{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용할 장치: {device}\")\n",
    "\n",
    "print(\"데이터를 로드하는 중...\")\n",
    "df = pd.read_csv(\"item.csv\")\n",
    "\n",
    "def combine_text(row):\n",
    "    return f\"{row['title']} [SEP] {row['short_description']}\"\n",
    "\n",
    "print(\"제목과 설명을 하나의 텍스트로 결합하는 중...\")\n",
    "df['combined_text'] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# 차원 축소를 위한 선형 레이어 추가\n",
    "class BertWithProjection(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = outputs.pooler_output\n",
    "        return self.projection(pooled)\n",
    "\n",
    "print(\"BERT 모델과 토크나이저를 로드하는 중...\")\n",
    "bert_model = BertWithProjection(output_dim=256).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_combined_embedding(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True, \n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs)\n",
    "    return embedding.squeeze().cpu().numpy()\n",
    "\n",
    "print(\"설명+제목 텍스트 임베딩 생성 중...\")\n",
    "df['combined_embedding'] = [\n",
    "    get_combined_embedding(text, tokenizer, bert_model) \n",
    "    for text in tqdm(df['combined_text'], desc=\"설명+제목 임베딩 생성\")\n",
    "]\n",
    "\n",
    "print(\"태그와 카테고리를 인코딩하는 중...\")\n",
    "# category 처리: 콤마로 분리하여 좌우 공백 제거 후 리스트 생성\n",
    "df['categories_list'] = df['categories'].fillna('').apply(lambda x: [s.strip() for s in x.split(',')] if x != '' else [])\n",
    "# 고유 category 목록 생성\n",
    "unique_categories = sorted(set(itertools.chain.from_iterable(df['categories_list'])))\n",
    "cat2idx = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "print(\"Unique category count:\", len(unique_categories))\n",
    "# 각 row마다 category 인덱스 리스트 생성\n",
    "df['encoded_categories'] = df['categories_list'].apply(lambda cats: [cat2idx[cat] for cat in cats])\n",
    "\n",
    "# tag 처리: tag 정보도 같은 방식으로 변환 (열 이름이 tags_sum라고 가정)\n",
    "df['tags_list'] = df['tags_sum'].fillna('').apply(lambda x: [s.strip() for s in x.split(',')] if x != '' else [])\n",
    "unique_tags = sorted(set(itertools.chain.from_iterable(df['tags_list'])))\n",
    "tag2idx = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "print(\"Unique tag count:\", len(unique_tags))\n",
    "df['encoded_tags'] = df['tags_list'].apply(lambda tags: [tag2idx[tag] for tag in tags])\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "# 임베딩 레이어 초기화: 고유 클래스 개수를 이용\n",
    "tag_embedding_layer = EmbeddingLayer(num_embeddings=len(unique_tags), embedding_dim=64).to(device)\n",
    "category_embedding_layer = EmbeddingLayer(num_embeddings=len(unique_categories), embedding_dim=32).to(device)\n",
    "\n",
    "def get_embedding_from_indices(indices, layer):\n",
    "    if not indices:\n",
    "        return torch.zeros(layer.embedding.embedding_dim, device=device)\n",
    "    idx_tensor = torch.tensor(indices, dtype=torch.long, device=device)\n",
    "    emb = layer(idx_tensor)\n",
    "    return emb.mean(dim=0)\n",
    "\n",
    "df['tag_embedding'] = df['encoded_tags'].apply(lambda inds: get_embedding_from_indices(inds, tag_embedding_layer).cpu().detach().numpy())\n",
    "df['category_embedding'] = df['encoded_categories'].apply(lambda inds: get_embedding_from_indices(inds, category_embedding_layer).cpu().detach().numpy())\n",
    "\n",
    "# item_id와 임베딩 매핑 생성\n",
    "embeddings_df = pd.DataFrame({\n",
    "    'item_id': df['item_id'].values,\n",
    "    'combined_embedding': list(df['combined_embedding']),\n",
    "    'tag_embedding': list(df['tag_embedding']),\n",
    "    'category_embedding': list(df['category_embedding'])\n",
    "})\n",
    "\n",
    "print(\"최종 임베딩이 완료되었습니다!\")\n",
    "print(f\"최종 임베딩 shape: {embeddings_df.shape}\")\n",
    "print(\"\\n결과 예시:\")\n",
    "print(embeddings_df.head())\n",
    "\n",
    "# 최종 임베딩 저장\n",
    "print(\"최종 임베딩을 저장하는 중...\")\n",
    "embeddings_df.to_csv(\"item_embeddings.csv\", index=False)\n",
    "embeddings_df.to_pickle(\"item_embeddings.pkl\")\n",
    "print(\"최종 임베딩이 저장되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
