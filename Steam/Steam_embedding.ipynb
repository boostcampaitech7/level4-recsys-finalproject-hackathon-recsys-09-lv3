{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용할 장치: cuda\n",
      "데이터를 로드하는 중...\n",
      "BERT 모델과 토크나이저를 로드하는 중...\n",
      "제목과 설명을 BERT로 임베딩하는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "설명 임베딩: 100%|██████████| 37141/37141 [05:15<00:00, 117.77it/s]\n",
      "제목 임베딩: 100%|██████████| 37141/37141 [04:35<00:00, 134.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태그와 카테고리를 정수 인코딩하는 중...\n",
      "태그와 카테고리 임베딩 레이어를 초기화하는 중...\n",
      "태그와 카테고리 임베딩을 계산하는 중...\n",
      "모든 임베딩을 결합하는 중...\n",
      "최종 임베딩이 완료되었습니다!\n",
      "최종 임베딩 shape: (37141, 1632)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용할 장치: {device}\")\n",
    "\n",
    "print(\"데이터를 로드하는 중...\")\n",
    "df = pd.read_csv(\"item.csv\")\n",
    "\n",
    "def combine_text(row):\n",
    "    return f\"{row['title']} [SEP] {row['short_description']}\"\n",
    "\n",
    "print(\"제목과 설명을 하나의 텍스트로 결합하는 중...\")\n",
    "df['combined_text'] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# 차원 축소를 위한 선형 레이어 추가\n",
    "class BertWithProjection(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = outputs.pooler_output\n",
    "        return self.projection(pooled)\n",
    "\n",
    "print(\"BERT 모델과 토크나이저를 로드하는 중...\")\n",
    "bert_model = BertWithProjection(output_dim=256).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_combined_embedding(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True, \n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(**inputs)\n",
    "    return embedding.squeeze().cpu().numpy()\n",
    "\n",
    "print(\"설명+제목 텍스트 임베딩 생성 중...\")\n",
    "df['combined_embedding'] = [\n",
    "    get_combined_embedding(text, tokenizer, bert_model) \n",
    "    for text in tqdm(df['combined_text'], desc=\"설명+제목 임베딩 생성\")\n",
    "]\n",
    "\n",
    "print(\"태그와 카테고리를 정수 인코딩하는 중...\")\n",
    "tag_encoder = LabelEncoder()\n",
    "category_encoder = LabelEncoder()\n",
    "\n",
    "df['encoded_tags'] = tag_encoder.fit_transform(df['tags_sum'])\n",
    "df['encoded_categories'] = category_encoder.fit_transform(df['categories'])\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "print(\"태그와 카테고리 임베딩 레이어를 초기화하는 중...\")\n",
    "tag_embedding_layer = EmbeddingLayer(num_embeddings=len(tag_encoder.classes_), embedding_dim=64).to(device)\n",
    "category_embedding_layer = EmbeddingLayer(num_embeddings=len(category_encoder.classes_), embedding_dim=32).to(device)\n",
    "print(\"len(tag_encoder.classes_): \", len(tag_encoder.classes_))\n",
    "print(\"len(category_encoder.classes_): \", len(category_encoder.classes_))\n",
    "\n",
    "# 태그와 카테고리 임베딩 계산\n",
    "print(\"태그와 카테고리 임베딩을 계산하는 중...\")\n",
    "tag_embeddings = tag_embedding_layer(torch.tensor(df['encoded_tags'].values).to(device))\n",
    "category_embeddings = category_embedding_layer(torch.tensor(df['encoded_categories'].values).to(device))\n",
    "\n",
    "# item_id와 임베딩 매핑 생성\n",
    "print(\"item_id와 임베딩 매핑 생성...\")\n",
    "embeddings_df = pd.DataFrame({\n",
    "    'item_id': df['item_id'].values,\n",
    "    'tag_embedding': list(tag_embeddings.detach().cpu().numpy()),\n",
    "    'category_embedding': list(category_embeddings.detach().cpu().numpy())\n",
    "})\n",
    "\n",
    "print(\"최종 임베딩이 완료되었습니다!\")\n",
    "print(f\"최종 임베딩 shape: {embeddings_df.shape}\")\n",
    "print(\"\\n결과 예시:\")\n",
    "print(embeddings_df.head())\n",
    "\n",
    "# 최종 임베딩 저장\n",
    "print(\"최종 임베딩을 저장하는 중...\")\n",
    "embeddings_df.to_csv(\"item_embeddings.csv\", index=False)\n",
    "embeddings_df.to_pickle(\"item_embeddings.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
